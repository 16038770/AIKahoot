{\rtf1\ansi\ansicpg1252\cocoartf2509
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\fswiss\fcharset0 Arial-BoldMT;\f1\froman\fcharset0 Times-Roman;\f2\fswiss\fcharset0 ArialMT;
}
{\colortbl;\red255\green255\blue255;\red0\green0\blue0;}
{\*\expandedcolortbl;;\cssrgb\c0\c0\c0;}
\paperw11900\paperh16840\margl1440\margr1440\vieww10800\viewh8400\viewkind0
\deftab720
\pard\pardeftab720\sl400\partightenfactor0

\f0\b\fs29\fsmilli14667 \cf2 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Bayes Theorem gives us a formula for reasoning about:
\f1\b0\fs24 \
\pard\pardeftab720\sl400\partightenfactor0

\f2\fs29\fsmilli14667 \cf2 Dependent Events
\f1\fs24 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 \
\pard\pardeftab720\sl400\partightenfactor0

\f0\b\fs29\fsmilli14667 \cf2 During classification, a Naive Bayes classifier multiplies a likelihood by:
\f1\b0\fs24 \
\pard\pardeftab720\sl400\partightenfactor0

\f2\fs29\fsmilli14667 \cf2 A prior
\f1\fs24 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 \
\pard\pardeftab720\sl400\partightenfactor0

\f0\b\fs29\fsmilli14667 \cf2 Multiplying the likelihood by the prior gives:
\f1\b0\fs24 \
\pard\pardeftab720\sl400\partightenfactor0

\f2\fs29\fsmilli14667 \cf2 A value proportional to the posterior probability
\f1\fs24 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 \
\pard\pardeftab720\sl400\partightenfactor0

\f0\b\fs29\fsmilli14667 \cf2 The Naive Bayes class-conditional independence assumption says that features are
\f1\b0\fs24 \
\pard\pardeftab720\sl400\partightenfactor0

\f2\fs29\fsmilli14667 \cf2 Independent if they're conditioned on the same class value
\f1\fs24 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 \
\pard\pardeftab720\sl400\partightenfactor0

\f0\b\fs29\fsmilli14667 \cf2 Naive Bayes training involves calculating means and standard deviations for:
\f1\b0\fs24 \
\pard\pardeftab720\sl400\partightenfactor0

\f2\fs29\fsmilli14667 \cf2 Normal distributions
\f1\fs24 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 \
\pard\pardeftab720\sl400\partightenfactor0

\f0\b\fs29\fsmilli14667 \cf2 A Normal distribution's probability density function is:
\f1\b0\fs24 \
\pard\pardeftab720\sl400\partightenfactor0

\f2\fs29\fsmilli14667 \cf2 Never zero
\f1\fs24 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 \
\pard\pardeftab720\sl400\partightenfactor0

\f0\b\fs29\fsmilli14667 \cf2 To estimate a Normal distribution from my data, I need to:
\f1\b0\fs24 \
\pard\pardeftab720\sl400\partightenfactor0

\f2\fs29\fsmilli14667 \cf2 Calculate its mean and standard distribution
\f1\fs24 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 \
\pard\pardeftab720\sl400\partightenfactor0

\f0\b\fs29\fsmilli14667 \cf2 If I estimate Normal distributions to help me calculate the "likelihood", I will need:
\f1\b0\fs24 \
\pard\pardeftab720\sl400\partightenfactor0

\f2\fs29\fsmilli14667 \cf2 One per feature value per class
\f1\fs24 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 \
\pard\pardeftab720\sl400\partightenfactor0

\f0\b\fs29\fsmilli14667 \cf2 I can get a prior probability for a class by dividing the number of times the class occurred by
\f1\b0\fs24 \
\pard\pardeftab720\sl400\partightenfactor0

\f2\fs29\fsmilli14667 \cf2 Total number of the trials
\f1\fs24 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 \
\pard\pardeftab720\sl400\partightenfactor0

\f0\b\fs29\fsmilli14667 \cf2 As the number of training examples grows large, which has the fastest classification phase:
\f1\b0\fs24 \
\pard\pardeftab720\sl400\partightenfactor0

\f2\fs29\fsmilli14667 \cf2 Naive Bayes
\f1\fs24 \
}